# 第十一章-特征选择与稀疏学习

## 子集搜索与评价

对弈个学习任务来说，给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用。我们将属性成为“特征”，对当前学习任务有用的属性成为“相关特征”、没什么用的属性成为“无关特征”。从给定的特征集合中选择出相关特征自己的过程，称为“特征选择”。

特征选择是一个重要的“数据预处理”过程，在现实机器学习任务中，获得数据之后通常先进行特征选择，此后才训练学习器。

原因有两个：

1. 维数灾难问题。
2. 去除不相关特征往往会降低学习任务的难度。

需要注意的是，特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得好的性能。

- 无关特征：与当前学习任务无关的特征。
- 冗余特征：包含的信息能够从其他特征中推演出来，去除冗余特征可能能够减轻学习过程的负担，但是有些冗余特征也会十分有用。

子集搜索：



子集评价：

给定数据集 ${D}$，假定 ${D}$ 中第 ${i}$ 类样本所占的比例为 ${p_i\ (i = 1,2,\ldots,|y|)}$。假定样本属性均为离散型，对属性子集 ${A}$，假定根据其取值将 ${D}$ 分成了 ${V}$ 个子集 ${D^1, D^2, \ldots, D^V}$，对每个子集中样本在 ${A}$ 的取值上相同，于是我们可计算属性子集 ${A}$ 的信息增益：

$${Gain(A) = Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} End(D^v)}$$

其中，定义信息熵：

$${Ent(D) = -\sum_{i=1}^{|y|} p_k \log_2^{p_k}}$$

信息增益 ${Gain(A)}$ 越大，意味着特征子集 ${A}$ 包含的有助于分类信息越多。于是，对每个特征后选子集，我们可基于训练数据集 ${D}$ 来计算其信息增益，以此作为评价准则。

![](http://ofqm89vhw.bkt.clouddn.com/9a19890b79e1a8f100bbe15eaa9a951f.png)

## 嵌入式选择与 ${L_1}$ 正则化

嵌入式特征选择是将特征选择过程与学习器融为一体，两者在同一优化过程中完成，既在学习器训练过程中自动地进行了特征选择。

${L_1}$ 范数和 ${L_2}$ 范数正则化都有助于降低过拟合风险，但前者 ${L_1}$ 范数比后者更易于获得 “稀疏”解，即它求得的 ${w}$ 会有更少的非零分量。

![二维情况：${L_1}$ 正则化比 ${L_2}$ 正则化更易于得到稀疏解](http://ofqm89vhw.bkt.clouddn.com/bd3670427015d40094140488146b4766.png)

损失函数的解要在平方误差项与正则化项之间折中，即出现在途中平方误差等值线与正则化等值线相交处。采用 ${L_1}$ 范数时，平方误差项等值线与正则化等值线的焦点常出现在坐标轴上；而采用 ${L_2}$ 范数时，两者的交点常常出现在某个象限中。所以 ${L_1}$ 更容易得到稀疏解。

## 习题